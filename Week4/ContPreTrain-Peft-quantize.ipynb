{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dxvsh/LearningPytorch/blob/main/Week4/ContPreTrain-Peft-quantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b905fc9-2ee5-413c-bda3-6112b7af1763",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5b905fc9-2ee5-413c-bda3-6112b7af1763"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Continual Pretraining of Llama 3.2 1B</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d79f2ae-530e-4de1-9faa-3712b7d47018",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6d79f2ae-530e-4de1-9faa-3712b7d47018"
      },
      "source": [
        "In this notebook, we will continually pre-train the Llama 3.2 1B model on the Tamil subset of the Sangraha dataset from AI4Bharat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877f5972-5d6f-40ed-b6a9-95ef09b2213e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "877f5972-5d6f-40ed-b6a9-95ef09b2213e"
      },
      "source": [
        "We are going to use an L4 GPU with 48 GB of memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7680a8-fab2-45cf-82cf-3ba34215400d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "2f7680a8-fab2-45cf-82cf-3ba34215400d"
      },
      "source": [
        "Training all the model's parameters requires significant amount of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0991b95f-5512-473c-b6c7-602aec3d490b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0991b95f-5512-473c-b6c7-602aec3d490b"
      },
      "source": [
        "Therefore, we will train the model using **LoRa**, one of several parameter-efficient fine-tuning techniques\n",
        "\n",
        "According to the docs:\n",
        "> **LoRA** (Low Rank Adaptation for LLMs) : is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets > /dev/null"
      ],
      "metadata": {
        "id": "xQIWa6nmJQaJ"
      },
      "id": "xQIWa6nmJQaJ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58a711bc-3ea2-45b0-9a8e-2a89455d644b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "58a711bc-3ea2-45b0-9a8e-2a89455d644b"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import math\n",
        "import wandb\n",
        "\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e334d7b-1e8a-4597-8b71-dac9df7b1e7a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4e334d7b-1e8a-4597-8b71-dac9df7b1e7a"
      },
      "source": [
        "```python\n",
        "wandb.init(\n",
        "    project=\"DLP-W4-CPT-Node-1\",\n",
        "    config={\n",
        "        \"batch_size\":4,\n",
        "        \"dataset\": \"Sangraha\",\n",
        "    },\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40182e1d-581f-4748-a919-bb2e5a77aa53",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "40182e1d-581f-4748-a919-bb2e5a77aa53"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Load the dataset </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32adbb29-e719-4e4a-b7f5-23bf8248945d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "32adbb29-e719-4e4a-b7f5-23bf8248945d"
      },
      "source": [
        "Let's load a small portion of sangraha dataset from ai4bharat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d7cd2bb3-c206-42c2-a413-12562f04889d",
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7cd2bb3-c206-42c2-a413-12562f04889d",
        "outputId": "81f1c1fb-90fc-4edb-8ca0-db77074059c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['doc_id', 'text', 'type'],\n",
            "    num_rows: 149796\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset('ai4bharat/sangraha',data_files=\"https://huggingface.co/datasets/ai4bharat/sangraha/resolve/main/verified/tam/data-0.parquet\")['train']\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_vmvLwcNO1w",
        "outputId": "35972754-3286-4e9c-ea3b-bcc1505f3685"
      },
      "id": "2_vmvLwcNO1w",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doc_id': '6e9d2be2015727c4f1590a67b5a200854cf08771',\n",
              " 'text': 'செய்முறைஃ\\nபச்சரிசி மற்றும் பச்சைப்பயறை ஒன்றாக சேர்த்து ஒரு மணி நேரம் ஊற வைக்கவும். ஊறிய அரிசி, பயறுடன், தேங்காய் துருவல், காய்ந்த மிளகாய், பெருங்காயத்தூள், இஞ்சி, கொத்தமல்லி, கறிவேப்பிலை, உப்பு சேர்த்து தோசை மாவு பதத்தில் அரைத்து கொள்ளவும்.\\n அடுப்பில் தோசைக்கல்லை வைத்து எண்ணெய் ஊற்றி காய்ந்ததும் அரைத்து வைத்திருக்கும் மாவை ஊற்றி சுட்டு எடுக்கவும். சுவையான பச்சை பயறு தோசை ரெசிபி ரெடி. ',\n",
              " 'type': 'web'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f727632c-2456-46d9-9b64-ca2141a725a8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f727632c-2456-46d9-9b64-ca2141a725a8"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Load Llama 3.2 1b tokenizer </h1>\n",
        "\n",
        "Note that currently access to the meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it.\n",
        "\n",
        "So don't run the below cells unless you have access to the model yet. You need to fill out a form and ask for access which is given in a few hours.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ea02b8-4b2c-44a8-823d-955e85ff15c3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "89ea02b8-4b2c-44a8-823d-955e85ff15c3",
        "outputId": "71856e6d-49a3-4ec5-b145-149899ee076a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 128000\n",
            "Context length: 131072\n"
          ]
        }
      ],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f'Vocab size: {tokenizer.vocab_size}')\n",
        "print(f'Context length: {tokenizer.model_max_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context length of the Llama 3.2 model is 131K tokens.\n",
        "\n",
        "Let's restrict it to 1024 for this demo."
      ],
      "metadata": {
        "id": "qZSBcMUbLIjd"
      },
      "id": "qZSBcMUbLIjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a347175f-11f1-4c19-a53b-91457b011a50",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "a347175f-11f1-4c19-a53b-91457b011a50"
      },
      "outputs": [],
      "source": [
        "tokenizer.model_max_length = 1024\n",
        "tokenizer.pad_token = tokenizer.eos_token # set the end of sentence token as the pad token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b1bf07-af61-4b43-bb34-6f66be78f51a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "a7b1bf07-af61-4b43-bb34-6f66be78f51a"
      },
      "source": [
        "Let's compute the approximate **fertility score** of the tokenizer for the Tamil language.\n",
        "\n",
        "The fertility score is a measure of how many tokens a word is split into.\n",
        "\n",
        "For example if a sample sentence has 10 words and after tokenization, it gets split into 50 tokens, then it means the fertility score is 50/10 = 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2605f5-95fb-41e1-ae8a-01958dcbaed6",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8e2605f5-95fb-41e1-ae8a-01958dcbaed6",
        "outputId": "92fa39bc-c172-484f-b00e-cfca15f22542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words: 47\n"
          ]
        }
      ],
      "source": [
        "example = ds[1]\n",
        "num_words = len(example['text'].split())\n",
        "print(f'Number of words: {num_words}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c145eae-4486-4bf3-8257-466b92a89b15",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0c145eae-4486-4bf3-8257-466b92a89b15",
        "outputId": "45fc8980-fbb6-45f4-f83b-2711bbd4b466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 521\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(example['text'])['input_ids']\n",
        "print(f'Number of tokens: {len(input_ids)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18616621-09b3-4259-ab87-8a43254b0e32",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "18616621-09b3-4259-ab87-8a43254b0e32",
        "outputId": "f378da38-2084-44a3-e99d-b1206f98364b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The fertility rate is: 11.085106382978724\n"
          ]
        }
      ],
      "source": [
        "print(f'The fertility rate is: {len(input_ids)/num_words}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08169993-52df-46cd-9cf3-e70f0089561b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "08169993-52df-46cd-9cf3-e70f0089561b"
      },
      "source": [
        "Typically, the **fertility score** for the tokenizer is quite high for **Indic languages.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a8cb3c-bb94-4252-adae-10212b7839d1",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c1a8cb3c-bb94-4252-adae-10212b7839d1"
      },
      "source": [
        "The fertility score is high (meaning that, **every word is split into 11 tokens on average**). Of course, to get the correct score, we have to use all the samples from the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0e465d-591d-4fa4-aa48-0318372feb37",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4f0e465d-591d-4fa4-aa48-0318372feb37",
        "outputId": "9ef52080-13c7-4e9f-a959-990be64b15c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "செய்முறைஃ\n",
            "பச்சரிசி மற்றும் பச்சைப்பயறை ஒன்றாக சேர்த்து ஒரு மணி நேரம் ஊற வைக்கவும். ஊறிய அரிசி, பயறுடன், தேங்காய் துருவல், காய்ந்த மிளகாய், பெருங்காயத்தூள், இஞ்சி, கொத்தமல்லி, கறிவேப்பிலை, உப்பு சேர்த்து தோசை மாவு பதத்தில் அரைத்து கொள்ளவும்.\n",
            " அடுப்பில் தோசைக்கல்லை வைத்து எண்ணெய் ஊற்றி காய்ந்ததும் அரைத்து வைத்திருக்கும் மாவை ஊற்றி சுட்டு எடுக்கவும். சுவையான பச்சை பயறு தோசை ரெசிபி ரெடி. \n"
          ]
        }
      ],
      "source": [
        "print(example['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58afffe-d819-4a93-9a6a-6f48e6ee26b6",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f58afffe-d819-4a93-9a6a-6f48e6ee26b6",
        "outputId": "5f314a1f-2086-4eb0-8a5c-4ff8932389d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|begin_of_text|>', 'à®', 'ļ', 'à¯', 'Ĩ', 'à®', '¯', 'à¯įà®', '®', 'à¯ģ', 'à®', '±', 'à¯', 'Ī', 'à®', 'ĥ', 'Ċ', 'à®', 'ª', 'à®', 'ļ', 'à¯įà®', 'ļ', 'à®', '°', 'à®¿à®', 'ļ', 'à®¿', 'Ġà®', '®', 'à®', '±', 'à¯įà®', '±', 'à¯ģ', 'à®', '®', 'à¯į', 'Ġà®', 'ª', 'à®', 'ļ', 'à¯įà®', 'ļ', 'à¯', 'Ī', 'à®']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|>செய்முறைஃ\\nபச்சரிசி மற்றும் பச்சைப்பயறை ஒன்றாக சேர்த்து ஒரு மணி நேரம் ஊற வைக்கவும். ஊறிய அரிசி, பயறுடன், தேங்காய் துருவல், காய்ந்த மிளகாய், பெருங்காயத்தூள், இஞ்சி, கொத்தமல்லி, கறிவேப்பிலை, உப்பு சேர்த்து தோசை மாவு பதத்தில் அரைத்து கொள்ளவும்.\\n அடுப்பில் தோசைக்கல்லை வைத்து எண்ணெய் ஊற்றி காய்ந்ததும் அரைத்து வைத்திருக்கும் மாவை ஊற்றி சுட்டு எடுக்கவும். சுவையான பச்சை பயறு தோசை ரெசிபி ரெடி. '"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(tokens[0 7])\n",
        "tokenizer.decode(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cfe0e7-6199-4474-bd4d-facc9341bdd1",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "09cfe0e7-6199-4474-bd4d-facc9341bdd1"
      },
      "source": [
        "Anyway, let us go with this!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets tokenize the samples in the dataset, and remove all the unecessary columns.\n",
        "\n",
        "Because we only need the `token_ids` and the `attention_mask` to feed to the model."
      ],
      "metadata": {
        "id": "CqVXHanaOw_4"
      },
      "id": "CqVXHanaOw_4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf82e922-99c9-40d2-b446-7f567433dbca",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cf82e922-99c9-40d2-b446-7f567433dbca"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    example = tokenizer(example['text'],padding=False,truncation=True)\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa57b89a-9664-48d1-a731-855ed7efaf1a",
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cd7bdc8c9a624b0ba8075e09e11c4187"
          ]
        },
        "id": "aa57b89a-9664-48d1-a731-855ed7efaf1a",
        "outputId": "2a283c3a-9473-4787-8b49-ab31478df828"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd7bdc8c9a624b0ba8075e09e11c4187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=12):   0%|          | 0/149796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 149796\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tokenized_ds = ds.map(tokenize,batched=True,num_proc=12, remove_columns=['doc_id', 'text', 'type'])\n",
        "print(tokenized_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is tokenized successfully. And we now have the `input_ids` and the `attention_mask`.\n",
        "\n",
        "Now just like we did in the previous notebooks, we concatenate all the `input_ids` and chunk them so that each of them becomes 1024 in length."
      ],
      "metadata": {
        "id": "gtmRfEiDPOjc"
      },
      "id": "gtmRfEiDPOjc"
    },
    {
      "cell_type": "markdown",
      "id": "ab1d57b8-dd30-4cf5-8d21-63cdd45b46a5",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ab1d57b8-dd30-4cf5-8d21-63cdd45b46a5"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Packing Sequence </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a374f2d-6d25-4a34-8210-e287ce0f5d61",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6a374f2d-6d25-4a34-8210-e287ce0f5d61"
      },
      "outputs": [],
      "source": [
        "def concatenate_and_chunk(examples):\n",
        "    pass\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20766ddf-0356-4935-9c4d-27c678839897",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "20766ddf-0356-4935-9c4d-27c678839897"
      },
      "outputs": [],
      "source": [
        "ds_chunked = ds.map(concatenate_and_chunk,\n",
        "                    batch_size=1000,\n",
        "                    batched=True,\n",
        "                    num_proc=12,\n",
        "                    remove_columns=['doc_id', 'text', 'type']\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "051945f6-0501-49c9-add0-02671b2557fc",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "051945f6-0501-49c9-add0-02671b2557fc"
      },
      "outputs": [],
      "source": [
        "ds_chunked.save_to_disk('tamil_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9d06ab-090b-413c-ba4b-8c0f9e6c759a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3c9d06ab-090b-413c-ba4b-8c0f9e6c759a"
      },
      "outputs": [],
      "source": [
        "ds_chunked = load_from_disk('tamil_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ddded4-3800-45a6-974f-71862848dffa",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "58ddded4-3800-45a6-974f-71862848dffa",
        "outputId": "41c45c9e-282f-44e9-9d2c-20f7f63e21d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 483683\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds_chunked)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that after concatenating and packing the samples, the dataset has gone up from about 150K samples to about 483K.\n",
        "\n",
        "Now, split the chunked dataset into train and test splits:"
      ],
      "metadata": {
        "id": "MbfSKaL-RXi-"
      },
      "id": "MbfSKaL-RXi-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8aeb03-b267-4ca7-a218-17c54766e9b3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8f8aeb03-b267-4ca7-a218-17c54766e9b3",
        "outputId": "d8776fe1-82a8-4933-ab40-136246ebb800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 483199\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 484\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "ds_split = ds_chunked.train_test_split(test_size=0.001,seed=42)\n",
        "print(ds_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5562713-bd6e-48a1-9ca0-1257bdd2fd88",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c5562713-bd6e-48a1-9ca0-1257bdd2fd88"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Data Collator </h1>\n",
        "\n",
        "Our objective here is causal language modelling (which essentially, means next token prediction), so we turn off the masked language modelling objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fd6450-9455-4e92-b84b-6f43b2076ea6",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "44fd6450-9455-4e92-b84b-6f43b2076ea6"
      },
      "outputs": [],
      "source": [
        "# dataloader\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572a9557-eca3-412e-b47b-716fbf3d65b7",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "572a9557-eca3-412e-b47b-716fbf3d65b7"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> Loading LLama 3.2 1b Model </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bce62bb0-be63-4b0c-b4c3-5fa490b74d8d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bce62bb0-be63-4b0c-b4c3-5fa490b74d8d"
      },
      "source": [
        "LLama 3.2 is a gated model, and you need permission to access the model weights. <br>\n",
        "(**Note:** You will receive access an hour or two after submitting the form from your Hugging Face account.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27722790-4bc8-4a34-80ee-7ef5a5f90c6a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "27722790-4bc8-4a34-80ee-7ef5a5f90c6a"
      },
      "source": [
        "You can find details about the model, such as its architecture, performance, and more,  [here](https://huggingface.co/meta-llama/Llama-3.2-1B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6dafee8-6c29-4d3d-b0b2-16cc014ae650",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d6dafee8-6c29-4d3d-b0b2-16cc014ae650"
      },
      "source": [
        "Here are some important details about the Llama 3.2 1B model: <br>\n",
        "* Number of parameters:  **1 Billion** (actually, 1.23 Billion)\n",
        "* Context length: 128 K (actually, 131K)\n",
        "* Vocabulary size: 128 K\n",
        "* Input modalities: Multilingual\n",
        "* Token count: **9T**\n",
        "* Knowledge cutoff: Dec 2023\n",
        "* GPU clusters: **916K** GPU Hours on H100 80GB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a936bcd-2fba-4a6e-aa44-e1b21c287222",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8a936bcd-2fba-4a6e-aa44-e1b21c287222"
      },
      "source": [
        "Since we are continuing the pre-training process, we load the model with a CausalLM head.\n",
        "\n",
        "This will load the pretrained weights of the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a34011-2d18-4550-bc55-8bf0da1c4ad5",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03a34011-2d18-4550-bc55-8bf0da1c4ad5"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id,pad_token_id=tokenizer.eos_token_id)\n",
        "# pad_token needed in general, otherwise raises an error (makes sense!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9e3367-22f6-41ea-b596-58feee26697b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "1b9e3367-22f6-41ea-b596-58feee26697b"
      },
      "source": [
        "Let's look at the configuration and architecture details of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0cef34-25cf-4c03-94ef-5b41a90f3ba9",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "be0cef34-25cf-4c03-94ef-5b41a90f3ba9",
        "outputId": "d8e97377-a412-4a2e-a76e-b97a325d04c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128001,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "configuration = model.config\n",
        "print(configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb9934b-0e03-43fe-9e74-5a612df2c223",
      "metadata": {
        "editable": true,
        "scrolled": true,
        "tags": [],
        "id": "0eb9934b-0e03-43fe-9e74-5a612df2c223",
        "outputId": "e68ee28d-fc47-4002-de0b-f620709b0605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefdb4e0-7870-4f22-adcb-78b3edc99d6e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cefdb4e0-7870-4f22-adcb-78b3edc99d6e"
      },
      "source": [
        "It uses decoder layers (like GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26278e58-c49a-4cfb-ba2b-3714de5d35ed",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "26278e58-c49a-4cfb-ba2b-3714de5d35ed"
      },
      "source": [
        "However, the model uses Relative Position Embeddings (RPE) that are added in the attention head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2459592-2735-4d17-ac07-1cbf35dd6d65",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "b2459592-2735-4d17-ac07-1cbf35dd6d65",
        "outputId": "9539f0b9-ddd1-42f1-d378-8ca6424c2b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Parameters:1.24 B\n"
          ]
        }
      ],
      "source": [
        "num_parameters = 0\n",
        "for param in model.parameters():\n",
        "    num_parameters += param.numel()\n",
        "print(f'Number of Parameters:{num_parameters/10**9:.2f} B')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bcf614e-c7f0-488b-abed-637323fa9bf3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4bcf614e-c7f0-488b-abed-637323fa9bf3"
      },
      "source": [
        "So, the total number of parameters is 1.24B.\n",
        "\n",
        "Let's calculate the memory requirement for this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9693e48-2329-4d93-a3ce-c54fcf54f1ec",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d9693e48-2329-4d93-a3ce-c54fcf54f1ec",
        "outputId": "f8ac62eb-972f-40e5-98b5-a990b7aa4fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "print(model.dtype) # 4 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e0735-6596-4f71-8fff-cf33f5f9065e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "783e0735-6596-4f71-8fff-cf33f5f9065e"
      },
      "source": [
        "The dataype is float32 which uses 4 bytes of memory per parameter.\n",
        "\n",
        "Therefore, the memory requirement is simply the number of parameters multiplied by the data type used to store each parameter. <br> (**Note:** We also need to store additional parameters, such as statistics for normalization, that do not require gradients)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c99ec89-d476-4a2e-b5ce-1f729ffffa6c",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "2c99ec89-d476-4a2e-b5ce-1f729ffffa6c",
        "outputId": "03640e5d-37a6-4a08-ae4a-497a55edfe2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.9432576\n"
          ]
        }
      ],
      "source": [
        "mem_in_gb = (num_parameters*4)/1e9  # divide by 10^9 to get the size in GB\n",
        "print(mem_in_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d44ca00-1d28-40e2-9865-162f3f51e0cb",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9d44ca00-1d28-40e2-9865-162f3f51e0cb"
      },
      "source": [
        "We can also get the info directly using a built-in function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cc882b-1acd-440f-9fd4-7866a7819f68",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "92cc882b-1acd-440f-9fd4-7866a7819f68",
        "outputId": "aea6be40-df57-40b3-de5f-1a3c560e7b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.943259776\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint()/1e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050c7ac0-6fd2-4779-9709-6e7a46bf3c4d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "050c7ac0-6fd2-4779-9709-6e7a46bf3c4d"
      },
      "source": [
        "* The model needs about 5GB of memory.\n",
        "* However, during training, **additional memory** is needed for storing gradients, which depends on the type of optimizer used.\n",
        "* Additionally, GPU kernels consume some memory, typically between 2 to 4 GB, depending on the type of GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28103ee-296d-4a5d-b785-8db646e967f9",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d28103ee-296d-4a5d-b785-8db646e967f9",
        "outputId": "8d60bee7-104f-4bbb-d679-045242fa9ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Memory requirement per sample: 20.7730304 GB\n"
          ]
        }
      ],
      "source": [
        "param_model = (num_parameters*4)/1e9\n",
        "adam_opt = 3*param_model # for storing moments\n",
        "kernel = 1\n",
        "bs = 1 # batch size\n",
        "print(f'Total Memory requirement per sample: {(param_model+adam_opt+kernel)*bs} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04767aff-4217-41f2-b4a1-f4c96c7b86e1",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "04767aff-4217-41f2-b4a1-f4c96c7b86e1"
      },
      "source": [
        "So, We need at **least 21 GB of memory** to train the model with a **batch size of 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf69497-71f6-4319-949f-8f2d06067d1e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4bf69497-71f6-4319-949f-8f2d06067d1e"
      },
      "source": [
        "Therefore, for this demonstration, we will use a single-node L4 GPU with 2 GPU instances, each having 24 GB of GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b241d693-e656-49d3-91b1-1f0ce11420dc",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "b241d693-e656-49d3-91b1-1f0ce11420dc"
      },
      "source": [
        "How do we increase the batch size per GPU device from 1 to at least 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f0e819-c73b-4717-9535-dd3ca0560b10",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "56f0e819-c73b-4717-9535-dd3ca0560b10"
      },
      "source": [
        "Of course, we can use a gradient accumulation strategy; however, this will increase the training time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8267b819-64bd-4917-aaa7-35961a0bb34d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8267b819-64bd-4917-aaa7-35961a0bb34d"
      },
      "source": [
        "The answer is : **PEFT** adapters (optionally combined with quantization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9507b0-00ce-42f2-9a23-720c18730860",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6d9507b0-00ce-42f2-9a23-720c18730860"
      },
      "source": [
        "<h1 style=\"color:Tomato;\"> PEFT: LoRA</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5f4f4e-dcc5-410a-bc7b-4fa0c035f3d0",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8e5f4f4e-dcc5-410a-bc7b-4fa0c035f3d0"
      },
      "source": [
        "Before we continue with the pre-training, let's see how the model generates a coherent text based on the given prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218253c5-00b3-481f-9564-ea7fe65f9fd8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "218253c5-00b3-481f-9564-ea7fe65f9fd8",
        "outputId": "0b9c1e86-17f5-4721-fdc1-af117452f73a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I was reading Feynman\\'s lecture on physics. He talks about 2 different ways to look at the world, the \"old\" way and the \"new\" way. In the \"old\" way, we look at the world and see that the universe is governed by laws. In the \"new\" way,']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"I was reading Feynman's lecture on physics. He talks about \"\n",
        "inputs = tokenizer(prompt,return_tensors='pt',padding=True)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=10, top_p=0.95)\n",
        "tokeni zer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the model starts to generate a pretty good and coherent response in English.\n",
        "\n",
        "But lets try the same thing for Tamil and see how good is the text generation:"
      ],
      "metadata": {
        "id": "YEkvJoopkNc0"
      },
      "id": "YEkvJoopkNc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294aa79b-ca65-4434-a975-de43716d4a9e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "294aa79b-ca65-4434-a975-de43716d4a9e",
        "outputId": "736267a1-6e93-4e38-891c-9d77c59de13f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை \\xa0கண்ணாடியில் பெய்யும் வரை இருக்கிறது. இதையால் புறநகர்ப் பகுதிகள் அதிக விளை']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை \"\n",
        "inputs = tokenizer(prompt,return_tensors='pt',padding=True)\n",
        "# set max tokens to a little higher as the words are split into 11 tokens on average\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fa1b36-c201-46d2-b3e3-3031aec55026",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "59fa1b36-c201-46d2-b3e3-3031aec55026"
      },
      "source": [
        "And although the model does output tamil, **the output text is not very coherent**. Likely because the pretrained model wasn't trained on a lot of Tamil data and therefore its understanding of the language is quite poor.\n",
        "\n",
        "<br>\n",
        "\n",
        "To make it better, let's continue the pre-training of the model on the Tamil subset of the Sangraha dataset using **LoRa** (a parameter efficient finetuning technique).\n",
        "\n",
        "> **LoRA** is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34485fb2",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "34485fb2"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/lora_1.png\" width=\"400\" height=\"360\">\n",
        "\n",
        "Thanks to **LoRa**, we don't need to update all the parameters of the model during fine tuning. We only need to update only a very small number of parameters, this makes the fine tuning process much more compute efficient.\n",
        "\n",
        "Here, the **Blue** side is the frozen pre-trained weights of the model. We don't need to update them any further. These are frozen.\n",
        "\n",
        "And the **Orange** side are the weights that will be updated. This is a low rank approximation to reduce the number of parameters that need to be trained. Only these parameters will be updated during fine-tuning.\n",
        "\n",
        "So thanks to LoRa, we only need to update a much smaller number of weights when we're interested in finetuning for a downstream task instead of updating all of the billions of weights in the models!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c48b8f-9d61-4677-adfe-7117046c1a6d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "25c48b8f-9d61-4677-adfe-7117046c1a6d"
      },
      "source": [
        "**Forward Pass:** <br>\n",
        "$$h=Wx+\\Delta Wx = Wx+BAx \\quad \\text{where} \\quad W \\in \\mathbb{R}^{d \\times k}, B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$$\n",
        "* $W$ is a pre-trained weight matrix  (during training, $W$ is frozen and **does not** receive gradient updates)\n",
        "* $A$ is initialized **randomly** (say, **Gaussian**)\n",
        "* $B$ is initialized to **zero**\n",
        "* $r$ is the rank for the low rank approximation\n",
        "* $\\Delta W$ is scaled by $\\frac{\\alpha}{r}$ after the first iteration, where $\\alpha$ is a constant\n",
        "\n",
        "Note that after matrix multiplication of the $A$ and $B$, the output dimensions are the same as the dimensions of the orignal pretrained weight matrix $W$. During fine-tuning, you only need to update the parameters in $A$ and $B$ (not the params in $W$).\n",
        "\n",
        "Also note, as the rank increases, the approximation gets better and better, but it comes at the cost of increased number of number of parameters to be fine tuned. So if you increase `r`, the number of parameters to be fine tuned increases.\n",
        "\n",
        "**Benefit of LoRa**: <br>\n",
        "* Switching between tasks only by swapping the LoRA weights instead of all the parameters.\n",
        "* This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM\n",
        "\n",
        "**Paper**: https://arxiv.org/pdf/2106.09685 <br>\n",
        "**HF Doc**: https://huggingface.co/docs/peft/v0.7.1/en/index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the **LoRa** config:"
      ],
      "metadata": {
        "id": "AuYDSjaJz1ux"
      },
      "id": "AuYDSjaJz1ux"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65896787-8850-4d92-ad43-cfa2e72a1738",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "65896787-8850-4d92-ad43-cfa2e72a1738"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, LoraModel\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # rank\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=TaskType.CAUSAL_LM, # this is a CLM task\n",
        "    inference_mode=False,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7448cbfe",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7448cbfe"
      },
      "source": [
        "In general, we can add adapters to any torch modules (nn.Linear, Conv1D,..). For example,\n",
        "```\n",
        "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a **LoRa** model and see the reduction in the number of trainable parameters:"
      ],
      "metadata": {
        "id": "sdMeszuR0oJ_"
      },
      "id": "sdMeszuR0oJ_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ffba50-4ce3-4bb5-91a8-319916f36d95",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "17ffba50-4ce3-4bb5-91a8-319916f36d95",
        "outputId": "627d7faa-283f-4ea5-ccef-05137405b862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
          ]
        }
      ],
      "source": [
        "from peft import get_peft_model\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c849e52e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c849e52e"
      },
      "source": [
        "This is an insane reduction! We had **1.2B** params in the original model and thanks to **LoRa**, we don't need to update all of them for fine tuning the model on the Tamil data. We only need to update just **1.7M** parameters for our fine tuning!\n",
        "\n",
        "That is a drastic change, we only need to train just 0.13% of the total parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f195cb",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03f195cb",
        "outputId": "f52d8f59-3949-4406-c387-01a6d09b54eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
            "        (layers): ModuleList(\n",
            "          (0-15): 16 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaAttention(\n",
            "              (q_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "              (v_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (rotary_emb): LlamaRotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229cfb43",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "229cfb43"
      },
      "source": [
        "From the above, observe that:\n",
        "\n",
        "* LoRA is only applied to the two projection layers: \"q_proj\" and \"v_proj\".\n",
        "* For \"q_proj\": $A \\in \\mathbb{R}^{2048 \\times 16}$ and $B \\in \\mathbb{R}^{16 \\times 2048}$ where the Rank is 16.\n",
        "* And if we multiply $A$ and $B$, the output dimensions will be: $2048 \\times 2048$, which is the dimension of \"q_proj\"\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "* For \"v_proj\": $A \\in \\mathbb{R}^{2048 \\times 16}$ and $B \\in \\mathbb{R}^{16 \\times 512}$ where the Rank is 16.\n",
        "* And if we multiply $A$ and $B$, the output dimensions will be: $2048 \\times 512$, which is the dimension of \"v_proj\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ba7ebe",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "e5ba7ebe"
      },
      "source": [
        "Once again, let us quickly verify the number of learnable parameters for our own satisfaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a7ddfc",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "34a7ddfc",
        "outputId": "ec6524dd-70d6-4f1a-bdfc-542662741991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Parameters of original model:1237518336 \n"
          ]
        }
      ],
      "source": [
        "num_parameters = 0\n",
        "for param in model.parameters():\n",
        "    num_parameters += param.numel()\n",
        "print(f'Number of Parameters of original model:{num_parameters} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b943a4",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "97b943a4",
        "outputId": "7fa36c39-7bd0-4557-824b-90d0f1cd1fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Parameters of LoRA model:1703936 \n"
          ]
        }
      ],
      "source": [
        "num_parameters_lora = 0\n",
        "for param in lora_model.parameters():\n",
        "    if param.requires_grad: # only count the parameters which need to be updated\n",
        "        num_parameters_lora += param.numel()\n",
        "print(f'Number of Parameters of LoRA model:{num_parameters_lora} ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is perfect and the math checks out! We got the same number of parameters as above using the built-in functions as well."
      ],
      "metadata": {
        "id": "PNMOZ8XX3Wht"
      },
      "id": "PNMOZ8XX3Wht"
    },
    {
      "cell_type": "markdown",
      "id": "4ae69c8b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4ae69c8b"
      },
      "source": [
        "Note, however, that we **still need to keep the entire model in GPU memory**. This requires about 5 GB of RAM. Additionally, we need to store the activation values of all layers, which consumes a significant amount of memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec48926a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ec48926a"
      },
      "source": [
        "Before proceeding further, let's check a few things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e9830f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "94e9830f",
        "outputId": "7c7b2688-c8fd-488e-8a31-d451ff77df30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-1B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=16, target_modules={'v_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lora_model.peft_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b45a25",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c1b45a25"
      },
      "source": [
        "<h1 style=\"color:Tomato;\">  Training </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the training arguments:"
      ],
      "metadata": {
        "id": "flh_7NB1933P"
      },
      "id": "flh_7NB1933P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1353bd23",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "1353bd23"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments( output_dir='lora_llama_1b_ct',\n",
        "                                  eval_strategy=\"steps\",\n",
        "                                  eval_steps=100,\n",
        "                                  num_train_epochs=1,\n",
        "                                  per_device_train_batch_size=2, # set the batch size to 2\n",
        "                                  per_device_eval_batch_size=2,\n",
        "                                  bf16=False,\n",
        "                                  fp16=True,\n",
        "                                  tf32=False,\n",
        "                                  gradient_accumulation_steps=1,\n",
        "                                  adam_beta1=0.9,\n",
        "                                  adam_beta2=0.999,\n",
        "                                  learning_rate=2e-5,\n",
        "                                  weight_decay=0.01,\n",
        "                                  logging_dir='logs',\n",
        "                                  logging_strategy=\"steps\",\n",
        "                                  logging_steps = 100,\n",
        "                                  save_steps=100,\n",
        "                                  save_total_limit=20,\n",
        "                                  report_to='none',\n",
        "                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the model trainer:"
      ],
      "metadata": {
        "id": "O-qJglyK973l"
      },
      "id": "O-qJglyK973l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7042618",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d7042618",
        "outputId": "ff8032cb-799b-48b5-ebde-05aac8fe41dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(model=lora_model, # note that we're only passing the lora_model for training, so only the lora adapter layers will get trained (and not all the params)\n",
        "                  args = training_args,\n",
        "                 train_dataset=ds_split[\"train\"],\n",
        "                 eval_dataset=ds_split[\"test\"],\n",
        "                 data_collator = data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4c4c47",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9b4c4c47"
      },
      "source": [
        "```python\n",
        "results = trainer.train()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d90801e5-d36a-42bc-9d1e-67ef35403fad",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d90801e5-d36a-42bc-9d1e-67ef35403fad"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/Lora_train_loss.png\" width=\"600\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c73011-714b-4ae2-97f4-b29ead7b3c3b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "a7c73011-714b-4ae2-97f4-b29ead7b3c3b"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/Lora_validation_loss.png\" width=\"600\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88eacd8-126c-4d63-b6ba-f7cfb48ac55b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d88eacd8-126c-4d63-b6ba-f7cfb48ac55b"
      },
      "source": [
        "The model used approximately 10 GB of GPU memory with a batch size of 1, which is less than half of what the original model requires (over 22 GB)\n",
        "\n",
        "So we're getting quite a good improvement using LoRa!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bf8582-7027-4d11-b25c-489cf4b9f9b9",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "53bf8582-7027-4d11-b25c-489cf4b9f9b9"
      },
      "source": [
        "**WARNING**\n",
        "* We loaded the original model weights and stored them in a variable `model`\n",
        "* We then applied LoRA and stored the resulting model in the variable `lora_model`\n",
        "* By design, **The `model` is modified `in-place`** (to save memory?)\n",
        "* This is not an issue when using a script; however, it can create problems in notebooks if we execute the `model` after executing `lora_model`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7238d6-e1ec-46f0-a0ba-b038dc2c6faf",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "fc7238d6-e1ec-46f0-a0ba-b038dc2c6faf"
      },
      "source": [
        "Let's load the model checkpoint (after processing 92,400 samples or 94 milllion tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "384bafc4-a5cf-47be-b7a7-2f4e6eb3adbe",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "384bafc4-a5cf-47be-b7a7-2f4e6eb3adbe"
      },
      "outputs": [],
      "source": [
        "model_cpt = AutoModelForCausalLM.from_pretrained('checkpoint-15400/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b560c3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "25b560c3",
        "outputId": "7a9ea9ba-e609-4780-abfb-1ee257551441"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை நிறுவனம் கூறுகிறது. பெரும்பாலான இடங்களில் கண மழை பெய்யும் காரணம் இது.']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானில\"\n",
        "inputs = tokenizer(prompt,return_tensors='pt',padding=True)\n",
        "# set max tokens to a little higher as the words are split into 11 tokens on average\n",
        "outputs = model_cpt.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finetuning, the text generation has gotten better and the output is atleast somewhat coherent (compared to the last time).\n",
        "\n",
        "And as we train on more samples and let the training run for more epochs, the generative capabilities on Tamil should get better."
      ],
      "metadata": {
        "id": "5iwpHiA0-aPu"
      },
      "id": "5iwpHiA0-aPu"
    },
    {
      "cell_type": "markdown",
      "id": "a42035ae-c575-4aff-a5bd-240922fb1113",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "a42035ae-c575-4aff-a5bd-240922fb1113"
      },
      "source": [
        "* As usual, we can store the model using the `.save_pretrained` method and load the peft model back with `from peft import PeftModel`\n",
        "* By default, the PeftModel is set for **inference**, but if you’d like to train the adapter further you can set `is_trainable=True.`\n",
        "```python\n",
        "lora_model = PeftModel.from_pretrained(model, \"path/to/model\", is_trainable=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a320d9-deb9-48a0-9e95-d2a8681e10ed",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "65a320d9-deb9-48a0-9e95-d2a8681e10ed"
      },
      "source": [
        "What happens to the model's ability to complete a given prompt coherently after continual pre-training on a potentially domain-specific dataset? In this case, We've tried fine-tuning the model on a Tamil dataset. **Does it retain its earlier world knowledge** and can it complete prompts just as it did before? Although we haven't trained the model on a larger dataset, we hope that it still retains its general knowledge. Let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb6c673",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6cb6c673"
      },
      "outputs": [],
      "source": [
        "prompt = \"I was reading Feynman's lecture on physics. He talks about \"\n",
        "inputs = tokenizer(prompt,return_tensors='pt',padding=True)\n",
        "outputs = model_cpt.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=10, top_p=0.95)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002a2786-1d53-4760-934b-c03826ed90c6",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "002a2786-1d53-4760-934b-c03826ed90c6"
      },
      "source": [
        "Following is the response we got: <br>\n",
        "[\"I was reading Feynman's lecture on physics. He talks about 2 different approaches to solving problems. One is to use the mathematics and the other is to use the physics. I have to say I think the physics approach is more useful, but I think the mathematics approach is also useful.\\nI think it is useful]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that finetuning the model on a domain-specific (Tamil, in this case) dataset hasn't disturbed the model's world knowledge. It can still answer the prompts as it did before."
      ],
      "metadata": {
        "id": "mxVo18Qz_x2K"
      },
      "id": "mxVo18Qz_x2K"
    },
    {
      "cell_type": "markdown",
      "id": "0acba171-62b2-4fec-ba22-e979248a92f1",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0acba171-62b2-4fec-ba22-e979248a92f1"
      },
      "source": [
        "<h1 style=\"color:Tomato;\">Quantization</h1>\n",
        "\n",
        "From the docs:\n",
        ">**Quantization** represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a8eb10-c91d-486d-a462-1ddb6530681f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "85a8eb10-c91d-486d-a462-1ddb6530681f"
      },
      "source": [
        "We can further reduce memory requirement (10 GB with LoRA) by quantizing the model parameters and adding adapters to the quantized model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd15af9a-89fe-4c55-ab74-316282f290fc",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "dd15af9a-89fe-4c55-ab74-316282f290fc"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce41d10-c487-441c-b85d-22ca2375cb86",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "dce41d10-c487-441c-b85d-22ca2375cb86"
      },
      "source": [
        "Load the model parameters in **8bit precision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf8dafe-636e-47dc-9579-e35a1d220a1c",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "caf8dafe-636e-47dc-9579-e35a1d220a1c"
      },
      "outputs": [],
      "source": [
        "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config,\n",
        "                                                  pad_token_id=tokenizer.eos_token_id,\n",
        "                                                  device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b245de03-5a26-4d92-9f4b-732d9fd680c4",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "b245de03-5a26-4d92-9f4b-732d9fd680c4",
        "outputId": "1b996721-52d7-4439-b921-311aad9c99c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model_8bit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8baf95",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5b8baf95"
      },
      "source": [
        "Let's add adapters to fine-tune the quantized model like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f973a448",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f973a448"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType,LoraModel\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c84dc0",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "23c84dc0",
        "outputId": "3b010904-9a0c-4422-838a-df29704f44eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
          ]
        }
      ],
      "source": [
        "from peft import get_peft_model\n",
        "lora_model = get_peft_model( , lora_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f285ff78",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f285ff78",
        "outputId": "42420aa0-eae1-479d-fda1-e6cca34ce1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
            "        (layers): ModuleList(\n",
            "          (0-15): 16 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaAttention(\n",
            "              (q_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
            "              (v_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
            "              (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
            "              (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (rotary_emb): LlamaRotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e22c685a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "e22c685a"
      },
      "source": [
        "Now we can train the model by using `Trainer` API!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748bcb3e",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "748bcb3e"
      },
      "source": [
        "* This approach requires approximately **6 GB** of GPU memory (with a batch size of 1)\n",
        "* In contrast, LoRA without quantization requires about **10 GB** of GPU memory (also with a batch size of 1)\n",
        "\n",
        "With the help of Quantization, our memory usage has further dropped from 10GB to 6GB."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef659192",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ef659192"
      },
      "source": [
        "Finally, we were able to continue the pre-training of Llama 3.2 1B with a batch size of 16 on the L4 GPU node (thanks to LoRa and Quantization)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab99359b",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ab99359b"
      },
      "source": [
        "Next, we will explore how to perform task-specific fine-tuning of the pre-trained model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}